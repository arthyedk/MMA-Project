# -*- coding: utf-8 -*-
"""Predicting passenger satisfaction using Survey_Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-p3o1J2KpT0cz4n47o6nfeohylthJXLp

Problem Statement
Goal:
The goal of the problem is to predict whether a passenger was satisfied or not considering his/her overall experience of traveling on the Shinkansen Bullet Train.

Dataset: 

The problem consists of 1 dataset: Survey data. The survey data is aggregated data of surveys indicating the post-service experience. You are expected to treat the dataset as raw data and perform any necessary data cleaning/validation steps as required.

The data has been split into two groups and provided in the Dataset folder. The folder contains both train and test data separately.

    Train_Data
    Test_Data


Target Variable: Overall_Experience (1 represents ‘satisfied’, and 0 represents ‘not satisfied’)

The training set can be used to build your machine learning model. The training set has labels for the target column - Overall_Experience.

The testing set should be used to see how well your model performs on unseen data. For the test set, it is expected to predict the ‘Overall_Experience’ level for each participant.

Data Dictionary:

All the data is self-explanatory. The survey levels are explained in the Data Dictionary file.

Submission File Format: You will need to submit a CSV file with exactly 35,602 entries plus a header row. The file should have exactly two columns

    ID
    Overall_Experience (contains 0 & 1 values, 1 represents ‘Satisfied’, and 0 represents ‘Not Satisfied’)

Evaluation Criteria:

Accuracy Score: The evaluation metric is simply the percentage of predictions made by the model that turned out to be correct. This is also called the accuracy of the model. It will be calculated as the total number of correct predictions (True Positives + True Negatives) divided by the total number of observations in the dataset.
 
In other words, the best possible accuracy is 100% (or 1) and the worst possible accuracy 0%
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing Libraries
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

import lightgbm as lgb
from sklearn.model_selection import train_test_split

import scipy as sc
import numpy as np
import pandas as pd

# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns

import sklearn
from sklearn import preprocessing
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import keras 
import tensorflow


from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer

from sklearn.multioutput import MultiOutputClassifier

from sklearn.pipeline import Pipeline

from sklearn.model_selection import train_test_split

from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.metrics import accuracy_score,confusion_matrix, classification_report


from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

import time

from google.colab import drive
drive.mount('/content/drive')

features_df = pd.read_csv("/content/drive/MyDrive/ML Learning csv/Surveydata_train.csv")

test_df = pd.read_csv("/content/drive/MyDrive/ML Learning csv/Surveydata_test.csv")

"""###Inspecting the data"""

features_df

"""###Handling the missing values"""

# Commented out IPython magic to ensure Python compatibility.
#plot a histogram for each numerical attribute
# %matplotlib inline  
import matplotlib.pyplot as plt
features_df.hist(bins=50, figsize=(18,14))
plt.show()

sns.heatmap(features_df.isnull(),yticklabels=False,cbar=False,cmap='viridis')

#Propotion of missing values
null_count = features_df.isnull().sum()
null_percentage = round((features_df.isnull().sum()/features_df.shape[0])*100, 2)
null_df = pd.DataFrame({'column_name' : features_df.columns,'null_count' : null_count,'null_percentage': null_percentage})
null_df.reset_index(drop = True, inplace = True)
null_df.sort_values(by = 'null_percentage', ascending = False)

features_df.head()

features_df.dtypes != "object"

"""###Fill in missing data with nan and then impute the data"""

# Replace Blank values with DataFrame.replace() methods.
features_df = features_df.replace(r'^\s*$', np.nan, regex=True)
features_df

"""Fill in Nas with the mode(most frequent category per feature)

Performed on both training and testing data
"""

features_df["Seat_Comfort"].fillna("Acceptable", inplace = True)
test_df["Seat_Comfort"].fillna("Acceptable", inplace = True)

features_df["Arrival_Time_Convenient"].fillna("Good", inplace = True)
test_df["Arrival_Time_Convenient"].fillna("Good", inplace = True)

features_df["Catering"].fillna("Good", inplace = True)
test_df["Catering"].fillna("Good", inplace = True)

features_df["Platform_Location"].fillna("Manageable", inplace = True)
test_df["Platform_Location"].fillna("Manageable", inplace = True)

features_df["Onboard_Wifi_Service"].fillna("Good", inplace = True)
test_df["Onboard_Wifi_Service"].fillna("Good", inplace = True)

features_df["Onboard_Entertainment"].fillna("Good", inplace = True)
test_df["Onboard_Entertainment"].fillna("Good", inplace = True)

features_df["Online_Support"].fillna("Good", inplace = True)
test_df["Online_Support"].fillna("Good", inplace = True)

features_df["Onboard_Service"].fillna("Good", inplace = True)
test_df["Onboard_Service"].fillna("Good", inplace = True)

features_df["Legroom"].fillna("Good", inplace = True)
test_df["Legroom"].fillna("Good", inplace = True)

features_df["Baggage_Handling"].fillna("Good", inplace = True)
test_df["Baggage_Handling"].fillna("Good", inplace = True)

features_df["Ease_of_Online_Booking"].fillna("Good", inplace = True)
test_df["Ease_of_Online_Booking"].fillna("Good", inplace = True)

features_df["CheckIn_Service"].fillna("Good", inplace = True)
test_df["CheckIn_Service"].fillna("Good", inplace = True)

features_df["Cleanliness"].fillna("Good", inplace = True)
test_df["Cleanliness"].fillna("Good", inplace = True)

features_df["Online_Boarding"].fillna("Good", inplace = True)
test_df["Online_Boarding"].fillna("Good", inplace = True)

features_df.isna().sum()

"""###Preprocessing the data

The missing values are now successfully handled.

There is still some minor but essential data preprocessing needed before we proceed towards building our machine learning model.

LabelEncoder can be used to normalize labels. It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels
"""

le = LabelEncoder()

features_df.Seat_Comfort = le.fit_transform(features_df.Seat_Comfort)
features_df.Arrival_Time_Convenient = le.fit_transform(features_df.Arrival_Time_Convenient)
features_df.Catering  = le.fit_transform(features_df.Catering )
features_df.Platform_Location = le.fit_transform(features_df.Platform_Location)
features_df.Onboard_Wifi_Service = le.fit_transform(features_df.Onboard_Wifi_Service)
features_df.Onboard_Entertainment = le.fit_transform(features_df.Onboard_Entertainment)
features_df.Online_Support = le.fit_transform(features_df.Online_Support)
features_df.Ease_of_Online_Bookingn = le.fit_transform(features_df.Ease_of_Online_Booking)
features_df.Onboard_Service = le.fit_transform(features_df.Onboard_Service)
features_df.Legroom = le.fit_transform(features_df.Legroom)
features_df.Baggage_Handling = le.fit_transform(features_df.Baggage_Handling)
features_df.CheckIn_Service  = le.fit_transform(features_df.CheckIn_Service )
features_df.Cleanliness = le.fit_transform(features_df.Cleanliness)
features_df.Online_Boarding = le.fit_transform(features_df.Online_Boarding)
features_df.Seat_Class = le.fit_transform(features_df.Seat_Class)
features_df.Ease_of_Online_Booking = le.fit_transform(features_df.Ease_of_Online_Booking)

test_df.Seat_Comfort = le.fit_transform(test_df.Seat_Comfort)
test_df.Arrival_Time_Convenient = le.fit_transform(test_df.Arrival_Time_Convenient)
test_df.Catering = le.fit_transform(test_df.Catering)
test_df.Platform_Location = le.fit_transform(test_df.Platform_Location)
test_df.Onboard_Wifi_Service = le.fit_transform(test_df.Onboard_Wifi_Service)
test_df.Onboard_Entertainment = le.fit_transform(test_df.Onboard_Entertainment)
test_df.Online_Support = le.fit_transform(test_df.Online_Support)
test_df.Ease_of_Online_Bookingn = le.fit_transform(test_df.Ease_of_Online_Booking)
test_df.Onboard_Service = le.fit_transform(test_df.Onboard_Service)
test_df.Legroom = le.fit_transform(test_df.Legroom)
test_df.Baggage_Handling = le.fit_transform(test_df.Baggage_Handling)
test_df.CheckIn_Service = le.fit_transform(test_df.CheckIn_Service)
test_df.Cleanliness = le.fit_transform(test_df.Cleanliness)
test_df.Online_Boarding = le.fit_transform(test_df.Online_Boarding)
test_df.Seat_Class = le.fit_transform(test_df.Seat_Class)
test_df.Ease_of_Online_Booking = le.fit_transform(test_df.Ease_of_Online_Booking)

"""###Correlation Plot"""

plt.figure(figsize = (10, 8))
sns.heatmap(features_df.corr())

"""###Selecting best features"""

from sklearn.feature_selection import SelectKBest, chi2, f_regression

"""Scikit-learn API provides SelectKBest class for extracting best features of given dataset. The SelectKBest method selects the features according to the k highest score."""

# Create the object for SelectKBest and fit and transform the classification data
# k is the number of features you want to select
bestfeatures = SelectKBest(score_func=chi2, k=13)

# Create feature and target variable for Classification problem
#SelectKBest selects the best 24 variables and drops the worst with least predictive power
x=features_df.iloc[:,0:17]
x= features_df.drop('Overall_Experience', axis=1)

y = features_df['Overall_Experience']

fit=bestfeatures.fit(x,y)

dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(x.columns)

#Concat two dataframes for better visualization 
featureScores = pd.concat([dfcolumns,dfscores],axis=1)

featureScores.columns = ['Specs','Score']  #naming the dataframe columns
print(featureScores.nlargest(13,'Score'))

#Worst features
#ID                         
#Seat_Class                 
#Cleanliness

"""Based on the results, I will drop ID, seat class and cleanliness in the training and testing sets because they don't help with our predictions"""

#Specify the dependent variable and independent variable
y = features_df['Overall_Experience']
x=features_df.drop(['ID','Seat_Class','Cleanliness','Overall_Experience'],axis=1)
x

test_df=test_df.drop(['ID','Seat_Class','Cleanliness'],axis=1)

categorical_cols = features_df.columns[features_df.dtypes == "object"].values
print(categorical_cols)

numeric_cols = features_df.columns[features_df.dtypes != "object"].values
print(numeric_cols)

"""###Scaling the data
Feature Scaling is a technique to standardize the independent features present in the data in a fixed range
"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
print(scaler.fit(features_df))
StandardScaler()

print(scaler.mean_)
print(scaler.transform(features_df))

scaler = StandardScaler()
print(scaler.fit(test_df))
StandardScaler()

print(scaler.mean_)
print(scaler.transform(test_df))

"""###  Fitting an XG Boost model to the train set Model of the Surveydata"""

from xgboost import XGBClassifier

clf = XGBClassifier(learning_rate=0.02, n_estimators=200, objective='binary:logistic',silent=True)

# train test split
X_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state=0)

clf.fit(X_train, y_train)

#Randomsearchparams = # specify your configurations as a dict
features_df[categorical_cols] = features_df[categorical_cols].astype('category')

parameters ={
              'booster':['gbtree','gblinear'],
              'learning_rate': [0.0001, 0.001, 0.01, 0.15, 0.25, 0.3], 
              'max_depth': [3,5,6,7],
              'min_child_weight': [12,15,17,19],
              'colsample_bytree': [0.8,0.9,1],
              "reg_alpha"   : [0.5,0.4,1],
              "reg_lambda"  : [2,3,5],
              "gamma"       : [4,5,6]
             
         } 
fit_params= {
    'feature_name': "auto",
    'categorical_feature': categorical_cols,
}

search = RandomizedSearchCV(clf,parameters, cv = 5, n_iter=80,verbose=1)

# Train on training data-
search.fit(X_train, y_train,verbose=1)

y_pred = search.predict(X_test)
print(classification_report(y_test, y_pred))

search.best_params_

# Train data prediction
y_hat = search.predict(X_train)

preds = search.predict_proba(X_test)
preds

# Train data confusion matrix
pd.crosstab(y_train, y_hat)

# Validation data confusion matrix
y_hat_val = search.predict(X_test)
pd.crosstab(y_test, y_hat_val)

# Predicting probs for final data
y_hat_test = search.predict_proba(test_df)

from sklearn.metrics import mean_squared_error

roc_auc_score(y_test, y_hat_val)

"""Evaluation Criteria:

Accuracy Score: The evaluation metric is simply the percentage of predictions made by the model that turned out to be correct. This is also called the accuracy of the model. It will be calculated as the total number of correct predictions (True Positives + True Negatives) divided by the total number of observations in the dataset.

"""

#(12091+14319)/(12091+828+1076+14319)

from sklearn import metrics
print(metrics.accuracy_score(y_test, y_hat_val))

"""# Submisison data"""

# importing submision data
submission_df = pd.read_csv("/content/drive/MyDrive/ML Learning csv/Sample_Submission.csv",index_col="ID")

submission_df.to_csv('my_submission.csv', index=True)

# filling with our predictied probabilities
submission_df['Overall_Experience'] = y_hat_test[:, 1]

# Saving it
#submission_df.to_csv('my_submission_lgbm.csv', index=True)